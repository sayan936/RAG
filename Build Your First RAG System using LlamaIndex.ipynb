{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ACfLrpwlo8Nl"
   },
   "source": [
    "# Build Your First RAG System\n",
    "\n",
    "1. Data Ingestion.\n",
    "2. Indexing.\n",
    "3. Retriever.\n",
    "4. Response Synthesizer.\n",
    "5. Querying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the required packages by executing the below commands in either Anaconda Prompt (in Windows) or Terminal (in Linux or Mac OS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9291,
     "status": "ok",
     "timestamp": 1703361268107,
     "user": {
      "displayName": "Ravi Theja",
      "userId": "12148656718425770960"
     },
     "user_tz": -330
    },
    "id": "Elvu26cIedWC",
    "outputId": "529ae34f-17a9-4724-b391-f1803279dfed"
   },
   "source": [
    "pip install llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is recommonded to store the API keys in a '.env' file, separate from the code.\n",
    "Plesae follow the below steps.\n",
    "1. Create a text file with the name '.env'\n",
    "2. Enter your api key in this format OPENAI_API_KEY='sk-e8943u9ru4982............'\n",
    "3. Save and close the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, as shown below you can provide the path of the '.env' file to 'load_dotenv' method.\n",
    "This will load any API keys stored in the '.env' file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: llama-index-embeddings-azure-openai in c:\\users\\sayan.chakraborty\\appdata\\roaming\\python\\python38\\site-packages (0.1.11)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.11.post1 in c:\\users\\sayan.chakraborty\\appdata\\roaming\\python\\python38\\site-packages (from llama-index-embeddings-azure-openai) (0.10.58)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.2.0,>=0.1.3 in c:\\users\\sayan.chakraborty\\appdata\\roaming\\python\\python38\\site-packages (from llama-index-embeddings-azure-openai) (0.1.11)\n",
      "Requirement already satisfied: llama-index-llms-azure-openai<0.2.0,>=0.1.3 in c:\\users\\sayan.chakraborty\\appdata\\roaming\\python\\python38\\site-packages (from llama-index-embeddings-azure-openai) (0.1.10)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\program files\\python38\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (2.0.29)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (0.6.4)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\sayan.chakraborty\\appdata\\roaming\\python\\python38\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (2024.3.1)\n",
      "Requirement already satisfied: httpx in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (0.25.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (3.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (3.8.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (1.24.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (1.23.2)\n",
      "Requirement already satisfied: pandas in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (2.0.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (10.3.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (2.31.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (4.11.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (1.16.0)\n",
      "Requirement already satisfied: azure-identity<2.0.0,>=1.15.0 in c:\\users\\sayan.chakraborty\\appdata\\roaming\\python\\python38\\site-packages (from llama-index-llms-azure-openai<0.2.0,>=0.1.3->llama-index-embeddings-azure-openai) (1.17.1)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.1 in c:\\users\\sayan.chakraborty\\appdata\\roaming\\python\\python38\\site-packages (from llama-index-llms-azure-openai<0.2.0,>=0.1.3->llama-index-embeddings-azure-openai) (0.1.27)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\program files\\python38\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\program files\\python38\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\program files\\python38\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\program files\\python38\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\program files\\python38\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\program files\\python38\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (4.0.3)\n",
      "Requirement already satisfied: azure-core>=1.23.0 in c:\\program files\\python38\\lib\\site-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.2.0,>=0.1.3->llama-index-embeddings-azure-openai) (1.30.1)\n",
      "Requirement already satisfied: cryptography>=2.5 in c:\\program files\\python38\\lib\\site-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.2.0,>=0.1.3->llama-index-embeddings-azure-openai) (42.0.5)\n",
      "Requirement already satisfied: msal>=1.24.0 in c:\\users\\sayan.chakraborty\\appdata\\roaming\\python\\python38\\site-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.2.0,>=0.1.3->llama-index-embeddings-azure-openai) (1.30.0)\n",
      "Requirement already satisfied: msal-extensions>=0.3.0 in c:\\users\\sayan.chakraborty\\appdata\\roaming\\python\\python38\\site-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.2.0,>=0.1.3->llama-index-embeddings-azure-openai) (1.2.0)\n",
      "Requirement already satisfied: click in c:\\program files\\python38\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\program files\\python38\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\program files\\python38\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (2024.4.16)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\program files\\python38\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\program files\\python38\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (1.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\program files\\python38\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (2.7.0)\n",
      "Requirement already satisfied: sniffio in c:\\program files\\python38\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (1.3.1)\n",
      "Requirement already satisfied: certifi in c:\\program files\\python38\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\program files\\python38\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\program files\\python38\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (3.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\program files\\python38\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\program files\\python38\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\program files\\python38\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (2.2.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\program files\\python38\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (3.0.3)\n",
      "Requirement already satisfied: colorama in c:\\program files\\python38\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\program files\\python38\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\program files\\python38\\lib\\site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (3.21.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\program files\\python38\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\program files\\python38\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\program files\\python38\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (2024.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\program files\\python38\\lib\\site-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (1.2.1)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\program files\\python38\\lib\\site-packages (from azure-core>=1.23.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.2.0,>=0.1.3->llama-index-embeddings-azure-openai) (1.16.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\program files\\python38\\lib\\site-packages (from cryptography>=2.5->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.2.0,>=0.1.3->llama-index-embeddings-azure-openai) (1.16.0)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\program files\\python38\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (23.2)\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in c:\\users\\sayan.chakraborty\\appdata\\roaming\\python\\python38\\site-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.24.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.2.0,>=0.1.3->llama-index-embeddings-azure-openai) (2.8.0)\n",
      "Requirement already satisfied: portalocker<3,>=1.4 in c:\\users\\sayan.chakraborty\\appdata\\roaming\\python\\python38\\site-packages (from msal-extensions>=0.3.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.2.0,>=0.1.3->llama-index-embeddings-azure-openai) (2.10.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\program files\\python38\\lib\\site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in c:\\program files\\python38\\lib\\site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai) (2.18.1)\n",
      "Requirement already satisfied: pycparser in c:\\program files\\python38\\lib\\site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.2.0,>=0.1.3->llama-index-embeddings-azure-openai) (2.22)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\program files\\python38\\lib\\site-packages (from portalocker<3,>=1.4->msal-extensions>=0.3.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.2.0,>=0.1.3->llama-index-embeddings-azure-openai) (306)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: llama-index-llms-azure-openai in c:\\users\\sayan.chakraborty\\appdata\\roaming\\python\\python38\\site-packages (0.1.10)\n",
      "Requirement already satisfied: azure-identity<2.0.0,>=1.15.0 in c:\\users\\sayan.chakraborty\\appdata\\roaming\\python\\python38\\site-packages (from llama-index-llms-azure-openai) (1.17.1)\n",
      "Requirement already satisfied: httpx in c:\\program files\\python38\\lib\\site-packages (from llama-index-llms-azure-openai) (0.25.2)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.11.post1 in c:\\users\\sayan.chakraborty\\appdata\\roaming\\python\\python38\\site-packages (from llama-index-llms-azure-openai) (0.10.58)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.1 in c:\\users\\sayan.chakraborty\\appdata\\roaming\\python\\python38\\site-packages (from llama-index-llms-azure-openai) (0.1.27)\n",
      "Requirement already satisfied: azure-core>=1.23.0 in c:\\program files\\python38\\lib\\site-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (1.30.1)\n",
      "Requirement already satisfied: cryptography>=2.5 in c:\\program files\\python38\\lib\\site-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (42.0.5)\n",
      "Requirement already satisfied: msal>=1.24.0 in c:\\users\\sayan.chakraborty\\appdata\\roaming\\python\\python38\\site-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (1.30.0)\n",
      "Requirement already satisfied: msal-extensions>=0.3.0 in c:\\users\\sayan.chakraborty\\appdata\\roaming\\python\\python38\\site-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\program files\\python38\\lib\\site-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (4.11.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\program files\\python38\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (2.0.29)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (0.6.4)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\sayan.chakraborty\\appdata\\roaming\\python\\python38\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (2024.3.1)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (3.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (3.8.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (1.24.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (1.23.2)\n",
      "Requirement already satisfied: pandas in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (2.0.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (10.3.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (2.31.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (4.66.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (1.16.0)\n",
      "Requirement already satisfied: anyio in c:\\program files\\python38\\lib\\site-packages (from httpx->llama-index-llms-azure-openai) (4.3.0)\n",
      "Requirement already satisfied: certifi in c:\\program files\\python38\\lib\\site-packages (from httpx->llama-index-llms-azure-openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\program files\\python38\\lib\\site-packages (from httpx->llama-index-llms-azure-openai) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\program files\\python38\\lib\\site-packages (from httpx->llama-index-llms-azure-openai) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\program files\\python38\\lib\\site-packages (from httpx->llama-index-llms-azure-openai) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\program files\\python38\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-llms-azure-openai) (0.14.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\program files\\python38\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\program files\\python38\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\program files\\python38\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\program files\\python38\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\program files\\python38\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\program files\\python38\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (4.0.3)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\program files\\python38\\lib\\site-packages (from azure-core>=1.23.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (1.16.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\program files\\python38\\lib\\site-packages (from cryptography>=2.5->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (1.16.0)\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in c:\\users\\sayan.chakraborty\\appdata\\roaming\\python\\python38\\site-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.24.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (2.8.0)\n",
      "Requirement already satisfied: portalocker<3,>=1.4 in c:\\users\\sayan.chakraborty\\appdata\\roaming\\python\\python38\\site-packages (from msal-extensions>=0.3.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (2.10.1)\n",
      "Requirement already satisfied: click in c:\\program files\\python38\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\program files\\python38\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\program files\\python38\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (2024.4.16)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\program files\\python38\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (1.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\program files\\python38\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (2.7.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\program files\\python38\\lib\\site-packages (from anyio->httpx->llama-index-llms-azure-openai) (1.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\program files\\python38\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\program files\\python38\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (2.2.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\program files\\python38\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (3.0.3)\n",
      "Requirement already satisfied: colorama in c:\\program files\\python38\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\program files\\python38\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\program files\\python38\\lib\\site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (3.21.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\program files\\python38\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\program files\\python38\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\program files\\python38\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (2024.1)\n",
      "Requirement already satisfied: pycparser in c:\\program files\\python38\\lib\\site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (2.22)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\program files\\python38\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (23.2)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\program files\\python38\\lib\\site-packages (from portalocker<3,>=1.4->msal-extensions>=0.3.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (306)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\program files\\python38\\lib\\site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in c:\\program files\\python38\\lib\\site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-azure-openai) (2.18.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-embeddings-azure-openai\n",
    "%pip install llama-index-llms-azure-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cKlax-updNW-"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables from the .env file\n",
    "load_dotenv('.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = sayan-gpt35trubo16k\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"d92ec509f0724079b5902d66692f75d6\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://raviriturajrohansaurabhsayan.openai.azure.com/\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2024-02-01\"\n",
    "os.environ[\"PINECONE_API\"] = \"747f8283-52d1-4011-9ba7-82b53a2541ec\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This setup ensures that our API key remains secure and easily configurable. Always remember to keep your `.env` file secure and avoid including it in version control.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLtBXZ0xDtmQ"
   },
   "source": [
    "# Stage 1: Data Ingestion\n",
    "\n",
    "## Data Loaders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading the data from a PDF file. For this, we will use the SimpleDirectoryReader class from LlamaIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "gGfPPk4gBAkQ"
   },
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting llama-index-readers-file\n",
      "  Downloading llama_index_readers_file-0.1.31-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in c:\\program files\\python38\\lib\\site-packages (from llama-index-readers-file) (4.12.3)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.37.post1 in c:\\users\\sayan.chakraborty\\appdata\\roaming\\python\\python38\\site-packages (from llama-index-readers-file) (0.10.58)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in c:\\program files\\python38\\lib\\site-packages (from llama-index-readers-file) (4.2.0)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file)\n",
      "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\program files\\python38\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file) (2.5)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\program files\\python38\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (2.0.29)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (0.6.4)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\sayan.chakraborty\\appdata\\roaming\\python\\python38\\site-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (2024.3.1)\n",
      "Requirement already satisfied: httpx in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (0.25.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (3.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (3.8.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (1.24.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (1.23.2)\n",
      "Requirement already satisfied: pandas in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (2.0.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (10.3.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (2.31.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (4.11.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\program files\\python38\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\program files\\python38\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\program files\\python38\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\program files\\python38\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\program files\\python38\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\program files\\python38\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\program files\\python38\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (4.0.3)\n",
      "Requirement already satisfied: click in c:\\program files\\python38\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\program files\\python38\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\program files\\python38\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (2024.4.16)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\program files\\python38\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\program files\\python38\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (1.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\program files\\python38\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (2.7.0)\n",
      "Requirement already satisfied: sniffio in c:\\program files\\python38\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (1.3.1)\n",
      "Requirement already satisfied: certifi in c:\\program files\\python38\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\program files\\python38\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\program files\\python38\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (3.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\program files\\python38\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\program files\\python38\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\program files\\python38\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (2.2.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\program files\\python38\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (3.0.3)\n",
      "Requirement already satisfied: colorama in c:\\program files\\python38\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\program files\\python38\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\program files\\python38\\lib\\site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (3.21.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\program files\\python38\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\program files\\python38\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\program files\\python38\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (2024.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\program files\\python38\\lib\\site-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (1.2.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\program files\\python38\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (23.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\program files\\python38\\lib\\site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in c:\\program files\\python38\\lib\\site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (2.18.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\program files\\python38\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.37.post1->llama-index-readers-file) (1.16.0)\n",
      "Downloading llama_index_readers_file-0.1.31-py3-none-any.whl (38 kB)\n",
      "Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Installing collected packages: striprtf, llama-index-readers-file\n",
      "Successfully installed llama-index-readers-file-0.1.31 striprtf-0.0.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install llama-index-readers-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(input_files=['data/transformers.pdf']).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then check the type of the `documents` variable and the total number of pages read from the PDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the datatype and length of the loaded documents\n",
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of pages read from the PDF\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='f1e25bf7-93d1-4651-8ae0-de1ad644101d', embedding=None, metadata={'page_label': '1', 'file_name': 'transformers.pdf', 'file_path': 'data\\\\transformers.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-07-29', 'last_modified_date': '2024-07-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To understand the structure of the loaded documents, let's retrieve the first document, which corresponds to the first page of the PDF:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='ab068b18-42c2-4ac6-bcec-d81108d11c95', embedding=None, metadata={'page_label': '1', 'file_name': 'transformers.pdf', 'file_path': 'data/transformers.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-05-28', 'last_modified_date': '2024-05-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the first document (essentially the first page in the PDF)\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also access specific attributes of the document, such as its ID and metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f1e25bf7-93d1-4651-8ae0-de1ad644101d'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the ID of the first document\n",
    "documents[0].id_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f1e25bf7-93d1-4651-8ae0-de1ad644101d'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].doc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_label': '1',\n",
       " 'file_name': 'transformers.pdf',\n",
       " 'file_path': 'data\\\\transformers.pdf',\n",
       " 'file_type': 'application/pdf',\n",
       " 'file_size': 2215244,\n",
       " 'creation_date': '2024-07-29',\n",
       " 'last_modified_date': '2024-07-29'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the metadata of the first document\n",
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.comNoam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.comNiki Parmar∗\n",
      "Google Research\n",
      "nikip@google.comJakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.comAidan N. Gomez∗ †\n",
      "University of Toronto\n",
      "aidan@cs.toronto.eduŁukasz Kaiser∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗ ‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring significantly\n",
      "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
      "to-German translation task, improving over the existing best results, including\n",
      "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
      "our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n",
      "training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
      "best models from the literature. We show that the Transformer generalizes well to\n",
      "other tasks by applying it successfully to English constituency parsing both with\n",
      "large and limited training data.\n",
      "∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\n",
      "has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
      "attention and the parameter-free position representation and became the other person involved in nearly every\n",
      "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
      "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\n",
      "efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\n",
      "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\n",
      "our research.\n",
      "†Work performed while at Google Brain.\n",
      "‡Work performed while at Google Research.\n",
      "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\n"
     ]
    }
   ],
   "source": [
    "# Get the text content of the first document\n",
    "print(documents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zv9VQB-EdsEd"
   },
   "source": [
    "## Embedding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to prepare our document for embedding and interaction with a large language model. We will use the OpenAI API for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key=os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "api_version=os.environ[\"OPENAI_API_VERSION\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "RTOBfe1hc2zu"
   },
   "outputs": [],
   "source": [
    "# Embedding Model\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding model\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=\"sayan-textembeddingada002\",\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint[0],\n",
    "    api_version=api_version\n",
    ")\n",
    "# embed_model = OpenAIEmbedding(model=\"karan-textembeddingada002\") #'text-embedding-3-small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5UD_RkiXf7Cm"
   },
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, let's set up our large language model (LLM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "6q6O3wusigcW"
   },
   "outputs": [],
   "source": [
    "# LLM\n",
    "from llama_index.llms.azure_openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the large language model\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    model=\"gpt-35-turbo-16k\",\n",
    "    deployment_name=\"sayan-gpt35trubo16k\",\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint[0],\n",
    "    api_version=api_version\n",
    ")\n",
    "# llm = OpenAI(model= \"gpt-4o\") # 'gpt-3.5-turbo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vseCdqiFj7W0"
   },
   "source": [
    "# Stage 2: Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pinecone\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    ")\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create index\n",
    "from pinecone import Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "\n",
    "\n",
    "# initialize connection to pinecone\n",
    "pc = Pinecone(api_key=os.getenv('PINECONE_API'))\n",
    "\n",
    "\n",
    "# create the index if it does not exist already\n",
    "index_name = 'rag-llama'\n",
    "existing_indexes = [i.get('name') for i in pc.list_indexes()]\n",
    "\n",
    "if index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,\n",
    "        metric='cosine',\n",
    "        spec=ServerlessSpec(cloud='aws', region='us-east-1')\n",
    "    )\n",
    "\n",
    "# connect to the index\n",
    "pinecone_index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the `VectorStoreIndex` class to create an index from the loaded documents. We pass the document chunks, embedding model, and LLM to the `from_documents` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d0546b6a0294ad38e5f251254951007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upserted vectors:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an index from the documents using the embedding model and LLM\n",
    "#index = VectorStoreIndex.from_documents(documents) #, llm=llm)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=PineconeVectorStore(pinecone_index=pinecone_index)\n",
    ")\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llama_index.core.indices.vector_store.base.VectorStoreIndex"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oA9czIv0sqe_"
   },
   "source": [
    "# Stage 3: Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we set up a retriever to query our indexed documents. This allows us to retrieve relevant information based on our queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "8-E66LtRjgT4"
   },
   "outputs": [],
   "source": [
    "# Setting up the Index as Retriever\n",
    "retriever = index.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `as_retriever` method converts our index into a retriever, and the `retrieve` method allows us to query the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "foOrz7q-oAJl"
   },
   "outputs": [],
   "source": [
    "# Retrieve information based on the query \"What are Transformers?\"\n",
    "retrieved_nodes = retriever.retrieve(\"What is self attention?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the metadata of the retrieved nodes to understand the source of the information:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metadata provides details such as the page label, file name, file path, file type, and other relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_label': '1',\n",
       " 'file_name': 'transformers.pdf',\n",
       " 'file_path': 'data\\\\transformers.pdf',\n",
       " 'file_type': 'application/pdf',\n",
       " 'file_size': 2215244,\n",
       " 'creation_date': '2024-07-29',\n",
       " 'last_modified_date': '2024-07-29'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the metadata of the first retrieved node\n",
    "retrieved_nodes[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's access the ID of the first retrieved node, which is a unique identifier for the first node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d7e2bc09-8126-490b-ab2d-00469cf0804f'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the ID of the first retrieved node\n",
    "retrieved_nodes[0].id_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can access the node_id attribute, which typically holds the same value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d7e2bc09-8126-490b-ab2d-00469cf0804f'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the node_id of the first retrieved node\n",
    "retrieved_nodes[0].node_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's explore the `node` attribute of the retrieved node. This attribute contains a `TextNode` object, which holds all the relevant information extracted during the retrieval process: The `TextNode` object includes various details such as metadata and text content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "executionInfo": {
     "elapsed": 401,
     "status": "ok",
     "timestamp": 1703361342847,
     "user": {
      "displayName": "Ravi Theja",
      "userId": "12148656718425770960"
     },
     "user_tz": -330
    },
    "id": "EXpkVs2RoHsA",
    "outputId": "d3f9b5b0-d90c-43b6-b194-8d05c249aa97"
   },
   "outputs": [],
   "source": [
    "# Access the full node object of the first retrieved node\n",
    "#retrieved_nodes[0].node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also extract and inspect the text content of this node to understand the retrieved information better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1703361344340,
     "user": {
      "displayName": "Ravi Theja",
      "userId": "12148656718425770960"
     },
     "user_tz": -330
    },
    "id": "c5KctGWPLi7u",
    "outputId": "e463cbb7-d169-456c-bccb-ab15004f711a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.comNoam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.comNiki Parmar∗\n",
      "Google Research\n",
      "nikip@google.comJakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.comAidan N. Gomez∗ †\n",
      "University of Toronto\n",
      "aidan@cs.toronto.eduŁukasz Kaiser∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗ ‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring significantly\n",
      "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
      "to-German translation task, improving over the existing best results, including\n",
      "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
      "our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n",
      "training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
      "best models from the literature. We show that the Transformer generalizes well to\n",
      "other tasks by applying it successfully to English constituency parsing both with\n",
      "large and limited training data.\n",
      "∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\n",
      "has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
      "attention and the parameter-free position representation and became the other person involved in nearly every\n",
      "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
      "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\n",
      "efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\n",
      "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\n",
      "our research.\n",
      "†Work performed while at Google Brain.\n",
      "‡Work performed while at Google Research.\n",
      "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\n"
     ]
    }
   ],
   "source": [
    "# Access the text content of the first retrieved node\n",
    "print(retrieved_nodes[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_label': '13',\n",
       " 'file_name': 'transformers.pdf',\n",
       " 'file_path': 'data\\\\transformers.pdf',\n",
       " 'file_type': 'application/pdf',\n",
       " 'file_size': 2215244,\n",
       " 'creation_date': '2024-07-29',\n",
       " 'last_modified_date': '2024-07-29'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_nodes[1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Visualizations\n",
      "Input-Input Layer5\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governments\n",
      "have\n",
      "passed\n",
      "new\n",
      "laws\n",
      "since\n",
      "2009\n",
      "making\n",
      "the\n",
      "registration\n",
      "or\n",
      "voting\n",
      "process\n",
      "more\n",
      "difficult\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governments\n",
      "have\n",
      "passed\n",
      "new\n",
      "laws\n",
      "since\n",
      "2009\n",
      "making\n",
      "the\n",
      "registration\n",
      "or\n",
      "voting\n",
      "process\n",
      "more\n",
      "difficult\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "Figure 3: An example of the attention mechanism following long-distance dependencies in the\n",
      "encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\n",
      "the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\n",
      "the word ‘making’. Different colors represent different heads. Best viewed in color.\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_nodes[1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ty19sHbWxoEu"
   },
   "source": [
    "# Stage 4: Response Synthesis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to synthesize responses from our large language model (LLM). For this, we use the `get_response_synthesizer` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "TnLdxijaxw80"
   },
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the `get_response_synthesizer` function takes our LLM as an argument and returns a synthesizer object that will help generate coherent responses to our queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the response synthesizer with the LLM\n",
    "response_synthesizer = get_response_synthesizer(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orz-nHJYyz0u"
   },
   "source": [
    "## Stage 5: Query Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set up a query engine. This engine will allow us to query our indexed documents and receive synthesized responses from the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "EiHo7R3K0OH3"
   },
   "outputs": [],
   "source": [
    "# Create a query engine using the index, LLM, and response synthesizer\n",
    "query_engine = index.as_query_engine(response_synthesizer=response_synthesizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `as_query_engine` method from our index object to create a query engine, passing the LLM and response synthesizer as arguments.\n",
    "\n",
    "With our query engine ready, we can now query the LLM using natural language:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "dTCGOKvI1Zj_"
   },
   "outputs": [],
   "source": [
    "# Query the LLM using the query engine\n",
    "response = query_engine.query(\"What is self attention?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this command, we query the LLM with the question \"What are Transformers?\" and store the response in the `response` variable.\n",
    "\n",
    "To view the response generated by the LLM, we can access the `response` attribute:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1703361376718,
     "user": {
      "displayName": "Ravi Theja",
      "userId": "12148656718425770960"
     },
     "user_tz": -330
    },
    "id": "mTgwMcaJ1nhT",
    "outputId": "e571e28d-711d-4c45-e7f7-0fd882677ee6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Self-attention is a mechanism used in the Transformer network architecture. It allows the model to weigh the importance of different words in a sequence when processing the sequence. This attention mechanism helps the model capture dependencies between words that are far apart in the sequence, enabling it to better understand the context and meaning of the words. In self-attention, each word in the sequence is compared to every other word in the sequence, and the attention weights are computed based on the similarity between the words. This allows the model to focus on relevant words and ignore irrelevant ones, improving its ability to generate accurate and coherent outputs.'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the response from the LLM\n",
    "response.response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns the synthesized answer to our query.\n",
    "\n",
    "We can further analyze the response by checking its length and inspecting the source nodes used to generate it:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These commands provide the length of the response and the number of source nodes, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "683"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the length of the response\n",
    "len(response.response) # number of characters in the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1703361403705,
     "user": {
      "displayName": "Ravi Theja",
      "userId": "12148656718425770960"
     },
     "user_tz": -330
    },
    "id": "aRMcVB1nQBbp",
    "outputId": "f84a6390-2f65-4ada-d322-5a63097a5187"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of source nodes\n",
    "len(response.source_nodes)  # list of 2 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d7e2bc09-8126-490b-ab2d-00469cf0804f'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the ID and metadata of the first source node\n",
    "response.source_nodes[0].id_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_label': '1',\n",
       " 'file_name': 'transformers.pdf',\n",
       " 'file_path': 'data\\\\transformers.pdf',\n",
       " 'file_type': 'application/pdf',\n",
       " 'file_size': 2215244,\n",
       " 'creation_date': '2024-07-29',\n",
       " 'last_modified_date': '2024-07-29'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the ID and metadata of the second source node\n",
    "response.source_nodes[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e790622e-a105-49cc-a6a1-4f556876c335'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.source_nodes[1].id_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_label': '13',\n",
       " 'file_name': 'transformers.pdf',\n",
       " 'file_path': 'data\\\\transformers.pdf',\n",
       " 'file_type': 'application/pdf',\n",
       " 'file_size': 2215244,\n",
       " 'creation_date': '2024-07-29',\n",
       " 'last_modified_date': '2024-07-29'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.source_nodes[1].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRUMuoRK7qvt"
   },
   "source": [
    "# End to End RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this final section, we will integrate everything we have learned to create a complete end-to-end Retrieval-Augmented Generation (RAG) pipeline. This pipeline will read documents, index them, and allow us to query the indexed data using a large language model (LLM).\n",
    "\n",
    "Let's walk through the entire process step by step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First, we import the necessary libraries and load our documents from a specified directory. We use the `SimpleDirectoryReader` class from LlamaIndex to read all documents in the 'data' directory:\n",
    "\n",
    "\n",
    "- The `SimpleDirectoryReader` reads the documents in the 'data' directory and stores them in the `documents` variable.\n",
    "\n",
    "- Next, we initialize our large language model (LLM) and embedding model. For this demonstration, we assume that these models have already been initialized and are available as `llm` and `embed_model`:\n",
    "\n",
    "- With our documents and models ready, we proceed to create an index. This index will facilitate efficient retrieval of information from our documents. Here, we use the `VectorStoreIndex` class to create an index from the loaded documents, embedding model, and LLM.\n",
    "\n",
    "- We then set up a query engine that will allow us to query the indexed documents using natural language. The query engine is created from our index and LLM:\n",
    "\n",
    "- Finally, we use the query engine to ask a question and receive a response from the LLM. In this example, we query the different types of Transformer models:\n",
    "\n",
    "- The `query` method sends the question to the LLM, which retrieves relevant information from the indexed documents and synthesizes a response. The response is then printed to the console.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1806,
     "status": "ok",
     "timestamp": 1703361456294,
     "user": {
      "displayName": "Ravi Theja",
      "userId": "12148656718425770960"
     },
     "user_tz": -330
    },
    "id": "HhAb3o0l7wwD",
    "outputId": "e683a5ab-23ac-486f-ba11-0d4b51bd8499"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context information does not provide a direct answer to the query.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "\n",
    "# Load data from the specified directory\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "\n",
    "# Initialize LLM and embedding model (assumed to be pre-initialized)\n",
    "llm = llm\n",
    "embed_model = embed_model\n",
    "\n",
    "# Create an index from the documents using the embedding model and LLM\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model, llm=llm)\n",
    "\n",
    "# Create a query engine from the index and LLM\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "\n",
    "# Query the LLM and print the response\n",
    "print(query_engine.query(\"What are the different types of Transformer Models?\").response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional encodings are needed in the Transformer because the model architecture relies entirely on an attention mechanism to draw global dependencies between input and output. Unlike recurrent or convolutional neural networks, the Transformer does not have inherent knowledge of the position of each element in the sequence. Positional encodings provide a way to incorporate positional information into the input embeddings, allowing the model to differentiate between elements based on their position in the sequence. This is important for the model to accurately capture the sequential nature of the data and learn dependencies between different positions in the sequence.\n"
     ]
    }
   ],
   "source": [
    "print(query_engine.query(\"Why do we need positional encodings in transformer?\").response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Encoder and Decoder blocks in the Transformer are components of the model architecture. The Encoder is composed of a stack of identical layers, each containing two sub-layers. The first sub-layer is a multi-head self-attention mechanism, and the second sub-layer is a position-wise fully connected feed-forward network. The output of each sub-layer is passed through a residual connection and layer normalization. The Decoder, on the other hand, is also composed of a stack of identical layers. In addition to the two sub-layers present in each encoder layer, the decoder inserts a third sub-layer that performs multi-head attention over the output of the encoder stack. The self-attention sub-layer in the decoder stack is modified to prevent positions from attending to subsequent positions.\n"
     ]
    }
   ],
   "source": [
    "print(query_engine.query(\"What are Encoder and Decoder blocks in transformer?\").response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To generate document embeddings using the Transformer architecture, you should choose the encoder-decoder structure. The encoder maps an input sequence of symbol representations to a sequence of continuous representations, while the decoder generates an output sequence of symbols. This architecture allows for the generation of document embeddings by encoding the input document and decoding it to generate the desired embeddings.\n"
     ]
    }
   ],
   "source": [
    "query = \"If I want to generate document embeddings, then which type of Transformer Architecture I must choose?\"\n",
    "print(query_engine.query(query).response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"If I want to generate document embeddings, \n",
    "then which type of Transformer Architecture I must choose among Encoders, Decoders or Encoder-Decorder?\"\"\"\n",
    "\n",
    "print(query_engine.query(query).response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By following these steps, we have created a fully functional end-to-end RAG pipeline. This pipeline can ingest documents, index them, and answer natural language queries using a powerful combination of LlamaIndex and OpenAI's models. This demonstrates the practical application of RAG systems in extracting and synthesizing information from large datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
