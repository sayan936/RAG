{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cd80e2b-c406-402c-8794-10a9877826a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75cfe08a-7dd7-4491-9c20-9d1488c9b5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: tiktoken in c:\\program files\\python38\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\program files\\python38\\lib\\site-packages (from tiktoken) (2024.4.16)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\program files\\python38\\lib\\site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\program files\\python38\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\python38\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\program files\\python38\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python38\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "7a8b1489-affa-4b33-8b99-112193ddfb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.chains import ConversationalRetrievalChain,LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import create_history_aware_retriever, RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "0ee31c50-0781-4add-a7cb-c3ed08e01ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv('.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "759df4b4-3603-4ecc-84cc-ba7c4e7b1448",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = sayan-gpt35trubo16k\n",
    "api_key=os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "api_version=os.environ[\"OPENAI_API_VERSION\"]\n",
    "pine_cone_Api = os.environ[\"PINECONE_API_KEY\"]\n",
    "#os.environ[\"HUGGING_FACE_API\"] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "31892c8a-a5a3-4cbf-8788-feb491e6c999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: genai.pdf\n",
      "Loading: gpt.pdf\n",
      "Loading: In_context_learning.pdf\n",
      "Loading: RAG.pdf\n",
      "Loading: transformers.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Path to the directory containing the PDF files\n",
    "path = \"./data/data\"\n",
    "\n",
    "# Load all PDFs and extract text\n",
    "documents = []\n",
    "for pdf_file in os.listdir(path):\n",
    "    if pdf_file.endswith('.pdf'):\n",
    "        loader = PyPDFLoader(os.path.join(path, pdf_file))\n",
    "        print(f\"Loading: {pdf_file}\")\n",
    "        documents.extend(loader.load())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "bf4997ce-c88f-46c3-8c20-205a01a6eb37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "2898987d-101e-4891-a778-c5849a8dbfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import tiktoken\n",
    "\n",
    "# Initialize the tokenizer for the specific model\n",
    "encoding = tiktoken.encoding_for_model(\"text-embedding-ada-002\")\n",
    "\n",
    "# Initialize the RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # Set desired max chunk size in tokens\n",
    "    chunk_overlap=50,  # Set desired overlap in tokens between chunks\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],  # Split preferentially at paragraphs, then new lines, etc.\n",
    "    length_function=lambda text: len(encoding.encode(text))  # Use token length\n",
    ")\n",
    "\n",
    "# Assuming `documents` contains text content from 5 PDFs\n",
    "chunks = []\n",
    "for document in documents:\n",
    "    chunks.extend(text_splitter.split_text(document.page_content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "894b4ef5-341e-4551-82eb-d127502b818d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "a7953566-d4ef-4b12-9008-55330a5d69d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding model\n",
    "embed_model = AzureOpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment=\"sayan-textembeddingada002\",\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint[0],\n",
    "    api_version=api_version\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "214ea9b6-4458-41c5-b282-29c220eaf806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone, ServerlessSpec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "7079e898-2eae-4ac0-9e5d-677a19521558",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=pine_cone_Api)\n",
    "\n",
    "index_name = 'llm-bot-2'\n",
    "existing_indexes = [i.get('name') for i in pc.list_indexes()]\n",
    "\n",
    "if index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,\n",
    "        metric='cosine',\n",
    "        spec=ServerlessSpec(cloud='aws', region='us-east-1')\n",
    "    )\n",
    "\n",
    "# connect to the index\n",
    "pinecone_index = pc.Index(index_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "2fa79c4b-e555-41cb-a172-0d5d4b5a6507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 918}},\n",
       " 'total_vector_count': 918}"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone_index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "5b35d440-ed2d-48dd-bfc3-1bb976e1cea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed chunk 0\n",
      "Successfully processed chunk 1\n",
      "Successfully processed chunk 2\n",
      "Successfully processed chunk 3\n",
      "Successfully processed chunk 4\n",
      "Successfully processed chunk 5\n",
      "Successfully processed chunk 6\n",
      "Successfully processed chunk 7\n",
      "Successfully processed chunk 8\n",
      "Successfully processed chunk 9\n",
      "Successfully processed chunk 10\n",
      "Successfully processed chunk 11\n",
      "Successfully processed chunk 12\n",
      "Successfully processed chunk 13\n",
      "Successfully processed chunk 14\n",
      "Successfully processed chunk 15\n",
      "Successfully processed chunk 16\n",
      "Successfully processed chunk 17\n",
      "Successfully processed chunk 18\n",
      "Successfully processed chunk 19\n",
      "Successfully processed chunk 20\n",
      "Successfully processed chunk 21\n",
      "Successfully processed chunk 22\n",
      "Successfully processed chunk 23\n",
      "Successfully processed chunk 24\n",
      "Successfully processed chunk 25\n",
      "Successfully processed chunk 26\n",
      "Successfully processed chunk 27\n",
      "Successfully processed chunk 28\n",
      "Successfully processed chunk 29\n",
      "Successfully processed chunk 30\n",
      "Successfully processed chunk 31\n",
      "Successfully processed chunk 32\n",
      "Successfully processed chunk 33\n",
      "Successfully processed chunk 34\n",
      "Successfully processed chunk 35\n",
      "Successfully processed chunk 36\n",
      "Successfully processed chunk 37\n",
      "Successfully processed chunk 38\n",
      "Successfully processed chunk 39\n",
      "Successfully processed chunk 40\n",
      "Successfully processed chunk 41\n",
      "Successfully processed chunk 42\n",
      "Successfully processed chunk 43\n",
      "Successfully processed chunk 44\n",
      "Successfully processed chunk 45\n",
      "Successfully processed chunk 46\n",
      "Successfully processed chunk 47\n",
      "Successfully processed chunk 48\n",
      "Successfully processed chunk 49\n",
      "Successfully processed chunk 50\n",
      "Successfully processed chunk 51\n",
      "Successfully processed chunk 52\n",
      "Successfully processed chunk 53\n",
      "Successfully processed chunk 54\n",
      "Successfully processed chunk 55\n",
      "Successfully processed chunk 56\n",
      "Successfully processed chunk 57\n",
      "Successfully processed chunk 58\n",
      "Successfully processed chunk 59\n",
      "Successfully processed chunk 60\n",
      "Successfully processed chunk 61\n",
      "Successfully processed chunk 62\n",
      "Successfully processed chunk 63\n",
      "Successfully processed chunk 64\n",
      "Successfully processed chunk 65\n",
      "Successfully processed chunk 66\n",
      "Successfully processed chunk 67\n",
      "Successfully processed chunk 68\n",
      "Successfully processed chunk 69\n",
      "Successfully processed chunk 70\n",
      "Successfully processed chunk 71\n",
      "Successfully processed chunk 72\n",
      "Successfully processed chunk 73\n",
      "Successfully processed chunk 74\n",
      "Successfully processed chunk 75\n",
      "Successfully processed chunk 76\n",
      "Successfully processed chunk 77\n",
      "Successfully processed chunk 78\n",
      "Successfully processed chunk 79\n",
      "Successfully processed chunk 80\n",
      "Successfully processed chunk 81\n",
      "Successfully processed chunk 82\n",
      "Successfully processed chunk 83\n",
      "Successfully processed chunk 84\n",
      "Successfully processed chunk 85\n",
      "Successfully processed chunk 86\n",
      "Successfully processed chunk 87\n",
      "Successfully processed chunk 88\n",
      "Successfully processed chunk 89\n",
      "Successfully processed chunk 90\n",
      "Successfully processed chunk 91\n",
      "Successfully processed chunk 92\n",
      "Successfully processed chunk 93\n",
      "Successfully processed chunk 94\n",
      "Successfully processed chunk 95\n",
      "Successfully processed chunk 96\n",
      "Successfully processed chunk 97\n",
      "Successfully processed chunk 98\n",
      "Successfully processed chunk 99\n",
      "Successfully processed chunk 100\n",
      "Successfully processed chunk 101\n",
      "Successfully processed chunk 102\n",
      "Successfully processed chunk 103\n",
      "Successfully processed chunk 104\n",
      "Successfully processed chunk 105\n",
      "Successfully processed chunk 106\n",
      "Successfully processed chunk 107\n",
      "Successfully processed chunk 108\n",
      "Successfully processed chunk 109\n",
      "Successfully processed chunk 110\n",
      "Successfully processed chunk 111\n",
      "Successfully processed chunk 112\n",
      "Successfully processed chunk 113\n",
      "Successfully processed chunk 114\n",
      "Successfully processed chunk 115\n",
      "Successfully processed chunk 116\n",
      "Successfully processed chunk 117\n",
      "Successfully processed chunk 118\n",
      "Successfully processed chunk 119\n",
      "Successfully processed chunk 120\n",
      "Successfully processed chunk 121\n",
      "Successfully processed chunk 122\n",
      "Successfully processed chunk 123\n",
      "Successfully processed chunk 124\n",
      "Successfully processed chunk 125\n",
      "Successfully processed chunk 126\n",
      "Successfully processed chunk 127\n",
      "Successfully processed chunk 128\n",
      "Successfully processed chunk 129\n",
      "Successfully processed chunk 130\n",
      "Successfully processed chunk 131\n",
      "Successfully processed chunk 132\n",
      "Successfully processed chunk 133\n",
      "Successfully processed chunk 134\n",
      "Successfully processed chunk 135\n",
      "Successfully processed chunk 136\n",
      "Successfully processed chunk 137\n",
      "Successfully processed chunk 138\n",
      "Successfully processed chunk 139\n",
      "Successfully processed chunk 140\n",
      "Successfully processed chunk 141\n",
      "Successfully processed chunk 142\n",
      "Successfully processed chunk 143\n",
      "Successfully processed chunk 144\n",
      "Successfully processed chunk 145\n",
      "Successfully processed chunk 146\n",
      "Successfully processed chunk 147\n",
      "Successfully processed chunk 148\n",
      "Successfully processed chunk 149\n",
      "Successfully processed chunk 150\n",
      "Successfully processed chunk 151\n",
      "Successfully processed chunk 152\n",
      "Successfully processed chunk 153\n",
      "Successfully processed chunk 154\n",
      "Successfully processed chunk 155\n",
      "Successfully processed chunk 156\n",
      "Successfully processed chunk 157\n",
      "Successfully processed chunk 158\n",
      "Successfully processed chunk 159\n",
      "Successfully processed chunk 160\n",
      "Successfully processed chunk 161\n",
      "Successfully processed chunk 162\n",
      "Successfully processed chunk 163\n",
      "Successfully processed chunk 164\n",
      "Successfully processed chunk 165\n",
      "Successfully processed chunk 166\n",
      "Successfully processed chunk 167\n",
      "Successfully processed chunk 168\n",
      "Successfully processed chunk 169\n",
      "Successfully processed chunk 170\n",
      "Successfully processed chunk 171\n",
      "Successfully processed chunk 172\n",
      "Successfully processed chunk 173\n",
      "Successfully processed chunk 174\n",
      "Successfully processed chunk 175\n",
      "Successfully processed chunk 176\n",
      "Successfully processed chunk 177\n",
      "Successfully processed chunk 178\n",
      "Successfully processed chunk 179\n",
      "Successfully processed chunk 180\n",
      "Successfully processed chunk 181\n",
      "Successfully processed chunk 182\n",
      "Successfully processed chunk 183\n",
      "Successfully processed chunk 184\n",
      "Successfully processed chunk 185\n",
      "Successfully processed chunk 186\n",
      "Successfully processed chunk 187\n",
      "Successfully processed chunk 188\n",
      "Successfully processed chunk 189\n",
      "Successfully processed chunk 190\n",
      "Successfully processed chunk 191\n",
      "Successfully processed chunk 192\n",
      "Successfully processed chunk 193\n",
      "Successfully processed chunk 194\n",
      "Successfully processed chunk 195\n",
      "Successfully processed chunk 196\n",
      "Successfully processed chunk 197\n",
      "Successfully processed chunk 198\n",
      "Successfully processed chunk 199\n",
      "Successfully processed chunk 200\n",
      "Successfully processed chunk 201\n",
      "Successfully processed chunk 202\n",
      "Successfully processed chunk 203\n",
      "Successfully processed chunk 204\n",
      "Successfully processed chunk 205\n",
      "Successfully processed chunk 206\n",
      "Successfully processed chunk 207\n",
      "Successfully processed chunk 208\n",
      "Successfully processed chunk 209\n",
      "Successfully processed chunk 210\n",
      "Successfully processed chunk 211\n",
      "Successfully processed chunk 212\n",
      "Successfully processed chunk 213\n",
      "Successfully processed chunk 214\n",
      "Successfully processed chunk 215\n",
      "Successfully processed chunk 216\n",
      "Successfully processed chunk 217\n",
      "Successfully processed chunk 218\n",
      "Successfully processed chunk 219\n",
      "Successfully processed chunk 220\n",
      "Successfully processed chunk 221\n",
      "Successfully processed chunk 222\n"
     ]
    }
   ],
   "source": [
    "max_retries = 5\n",
    "for i, chunk in enumerate(chunks):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            # Process each chunk\n",
    "            PineconeVectorStore.from_texts(\n",
    "                texts=[chunk],  # Process one chunk at a time\n",
    "                index_name=index_name,\n",
    "                embedding=embed_model\n",
    "            )\n",
    "            print(f\"Successfully processed chunk {i}\")\n",
    "            break  # Break the loop if successful\n",
    "        except RateLimitError:\n",
    "            print(f\"Rate limit exceeded at chunk {i}. Retrying after delay...\")\n",
    "            retries += 1\n",
    "            delay = 60 * (2 ** retries)  # Exponential backoff: 1, 2, 4, 8, 16 minutes\n",
    "            print(f\"Waiting for {delay} seconds before retrying...\")\n",
    "            time.sleep(delay)  # Wait before retrying\n",
    "    if retries == max_retries:\n",
    "        print(f\"Max retries reached for chunk {i}. Skipping...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "c66e96c2-541e-4a41-9466-8285d2306361",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = PineconeVectorStore(pinecone_index, embed_model)\n",
    "# # Example user query\n",
    "# user_query = \"What are the benefits of transformers in machine learning?\"\n",
    "\n",
    "# # Retrieve relevant documents\n",
    "# results = retriever.get_relevant_documents(user_query)\n",
    "\n",
    "# # Display results\n",
    "# for i, result in enumerate(results):\n",
    "#     print(f\"Result {i + 1}:\")\n",
    "#     print(result.page_content)\n",
    "#     print(result.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "70ba8784-f481-4af8-bc07-c9427141f8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "fac00373-d28a-4a01-a526-6f78330b67f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    model=\"gpt-35-turbo-16k\",\n",
    "    deployment_name=\"sayan-gpt35trubo16k\",\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint[0],\n",
    "    api_version=api_version\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "f1d18f84-801b-4909-9477-a86d0a4303c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "# which might reference context in the chat history, formulate a standalone question \\\n",
    "# which can be understood without the chat history. Do NOT answer the question, \\\n",
    "# just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "# contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\", contextualize_q_system_prompt),\n",
    "#         MessagesPlaceholder(\"chat_history\"),\n",
    "#         (\"human\", \"{input}\"),\n",
    "#     ]\n",
    "# )\n",
    "# history_aware_retriever = create_history_aware_retriever(\n",
    "#     llm, vector_store.as_retriever(), contextualize_q_prompt\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "2929e9d9-30d3-4c97-a11b-042bb31cf10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', \"You are a helpful assistant that generates follow-up questions to deepen the conversation.\"),\n",
    "        ('system', \"Based on the previous response, suggest a relevant follow-up question.\"),\n",
    "        ('assistant', \"{response}\"),  # Placeholder for the assistant's response\n",
    "        ('human', \"Follow-up question:\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create an LLMChain to generate questions\n",
    "question_generator_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=question_prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "6efd79c8-a1eb-4090-996e-4672afbaf3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', \"You are a knowledgeable assistant specializing in generative AI.\"),\n",
    "        ('system', \"Your responses should be structured with bullet points and detailed explanations if necessary.Only provide response if the question is related to ai\"),\n",
    "        ('system', \"Here is the relevant information:\\n{context}\"),  # Placeholder for documents\n",
    "        ('human', \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Combine Docs Chain - to generate responses\n",
    "combine_docs_chain = load_qa_chain(\n",
    "    llm=llm,\n",
    "    chain_type = \"stuff\",\n",
    "    prompt=response_prompt_template\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "ce51e0a2-c8ef-48e8-a6c9-37b855451607",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the ConversationalRetrievalChain\n",
    "qa_chain = ConversationalRetrievalChain(\n",
    "    retriever=vector_store.as_retriever(),\n",
    "    combine_docs_chain=combine_docs_chain,\n",
    "    question_generator=question_generator_chain  # Use the custom question generator chain\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "36bce1f7-670a-4c5e-8960-859cda7ab303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "54e06bae-08aa-4d64-8311-2552289db791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The use of generative AI raises several ethical considerations, including:\n",
      "\n",
      "1. Bias and Discrimination: Generative AI systems are trained on existing data, which may contain biases and discriminatory patterns. If these biases are not addressed, the AI system may perpetuate and amplify them, leading to unfair outcomes and discrimination against certain individuals or groups.\n",
      "\n",
      "2. Privacy and Data Protection: Generative AI systems require large amounts of data to learn and generate content. This raises concerns about the privacy and security of personal data. It is important to ensure that user data is collected and used in a transparent and ethical manner, with proper consent and protection measures in place.\n",
      "\n",
      "3. Misinformation and Manipulation: Generative AI systems have the potential to create realistic and convincing content, including misinformation and deepfakes. This can be used to spread false information, manipulate public opinion, and deceive individuals. It is crucial to develop mechanisms to detect and mitigate the spread of such content.\n",
      "\n",
      "4. Accountability and Transparency: Generative AI systems can be complex and opaque, making it difficult to understand how they make decisions and generate content. This lack of transparency raises concerns about accountability and the ability to detect and address potential biases or errors in the system.\n",
      "\n",
      "5. Intellectual Property Rights: Generative AI systems can create content that may infringe on intellectual property rights, such as copyright or trademark. It is important to establish clear guidelines and regulations to address these issues and protect the rights of content creators.\n",
      "\n",
      "6. Job Displacement and Economic Impact: The automation of tasks through generative AI systems may lead to job displacement, particularly in industries that rely heavily on content creation or data analysis. This raises concerns about the economic impact on individuals and communities, and the need for retraining and support programs.\n",
      "\n",
      "7. Psychological and Social Impacts: The use of generative AI systems in areas such as social media or education can have psychological and social impacts on individuals. For example, the use of AI-generated content may affect trust, authenticity, and human connection. It is important to consider the potential consequences and address any negative impacts on individuals and society.\n",
      "\n",
      "To address these ethical considerations, it is crucial to have robust guidelines, regulations, and ethical frameworks in place. This includes ensuring diversity and inclusivity in the development and training of AI systems, promoting transparency and explainability, protecting privacy and data rights, and fostering collaboration between stakeholders to address these issues collectively.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "user_query = \"Discuss the ethical considerations associated with the use of generative AI.\"\n",
    "\n",
    "conversation_context = []\n",
    "\n",
    "response = qa_chain.run({\"question\": user_query, \"chat_history\": conversation_context})\n",
    "\n",
    "\n",
    "conversation_context.append({\"question\": user_query, \"answer\": response})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "c688c223-4e17-4940-978e-b54f28f2404a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Generative Adversarial Networks (GANs) can be challenging due to several factors. Here are some of the challenges and potential solutions to address them:\n",
      "\n",
      "1. Mode Collapse: Mode collapse occurs when the generator in a GAN fails to capture the full diversity of the data distribution and instead generates only a limited set of samples. This leads to repetitive and unrealistic outputs.\n",
      "\n",
      "   - Addressing mode collapse can be done by using techniques like minibatch discrimination, which encourages the generator to produce diverse samples. Another approach is to use techniques like Wasserstein GAN (WGAN) or its variants, which have been shown to be more stable and less prone to mode collapse.\n",
      "\n",
      "2. Vanishing/Exploding Gradients: GANs can suffer from vanishing or exploding gradients, which can make the training process unstable or lead to slow convergence.\n",
      "\n",
      "   - To address vanishing gradients, techniques like weight initialization, batch normalization, and gradient clipping can be used. These techniques help stabilize the gradient flow during training.\n",
      "\n",
      "3. Training Instability: GAN training can be unstable, with the generator and discriminator oscillating between different levels of performance and struggling to find an equilibrium.\n",
      "\n",
      "   - One solution is to use techniques like feature matching or minibatch discrimination, which provide additional stability to the training process. Another approach is to use progressive training, where the GAN is trained in stages, starting with low-resolution images and gradually increasing the resolution.\n",
      "\n",
      "4. Mode Dropping: Mode dropping occurs when the generator fails to capture some modes of the data distribution and produces samples that do not represent the entire dataset.\n",
      "\n",
      "   - To address mode dropping, techniques like self-attention mechanisms or spectral normalization can be used. These techniques help the generator capture more diverse modes of the data distribution.\n",
      "\n",
      "5. Evaluation and Metrics: Evaluating the quality of generated samples from a GAN can be challenging. Traditional metrics like pixel-wise similarity may not capture the visual quality or diversity of the generated samples.\n",
      "\n",
      "   - One solution is to use perceptual metrics like the Inception Score or Frechet Inception Distance (FID), which measure the quality and diversity of the generated samples based on feature representations from a pre-trained classifier. Other metrics like precision and recall can also be used to evaluate the GAN's performance.\n",
      "\n",
      "6. Dataset Bias: GANs can be sensitive to biases present in the training data, leading to biased or unfair generation.\n",
      "\n",
      "   - Addressing dataset bias involves careful curation of the training data to ensure a representative and diverse dataset. Techniques like data augmentation and balancing can also help mitigate bias in the training process.\n",
      "\n",
      "Overall, training GANs is an ongoing research area, and addressing these challenges requires a combination of algorithmic improvements, architectural choices, and careful training strategies.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "user_query = \"What are the challenges in training GANs, and how can they be addressed?\"\n",
    "\n",
    "conversation_context = []\n",
    "\n",
    "response = qa_chain.run({\"question\": user_query, \"chat_history\": conversation_context})\n",
    "\n",
    "conversation_context.append({\"question\": user_query, \"answer\": response})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "63de1939-d43e-4400-bde2-9d16ed17abe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Transformer is a deep learning model architecture that was introduced in the paper \"Attention Is All You Need\" by Vaswani et al. It is specifically designed for sequence-to-sequence tasks such as machine translation, text summarization, and language modeling. The Transformer model replaces traditional recurrent neural networks (RNNs) with self-attention mechanisms to capture dependencies between different positions in the input and output sequences. Here is a detailed explanation of the architecture and functioning of the Transformer model:\n",
      "\n",
      "1. Encoder-Decoder Structure:\n",
      "   - The Transformer model follows an encoder-decoder structure, commonly used in sequence transduction models. The encoder maps an input sequence to a continuous representation, and the decoder generates an output sequence based on this representation.\n",
      "   - The encoder and decoder both consist of multiple layers. Each layer has two sub-layers: a self-attention mechanism and a feed-forward neural network.\n",
      "   - The self-attention mechanism allows the model to capture dependencies between different positions in the input sequence. It computes weighted sums of the values at different positions, where the weights are determined by the similarity between a query and the keys.\n",
      "   - The feed-forward neural network processes the output of the self-attention mechanism and applies non-linear transformations to it.\n",
      "\n",
      "2. Encoder Architecture:\n",
      "   - The encoder is composed of a stack of N identical layers. Each layer consists of two sub-layers: a multi-head self-attention mechanism and a feed-forward neural network.\n",
      "   - The self-attention mechanism takes the input sequence and computes attention weights for each position. It captures how much each position attends to other positions in the input sequence.\n",
      "   - The feed-forward neural network applies non-linear transformations to the output of the self-attention mechanism.\n",
      "   - Residual connections and layer normalization are applied to each layer to facilitate the flow of information and improve training.\n",
      "\n",
      "3. Decoder Architecture:\n",
      "   - The decoder also consists of a stack of N identical layers. In addition to the two sub-layers present in the encoder layers, the decoder inserts a third sub-layer that performs multi-head attention over the output of the encoder stack.\n",
      "   - The self-attention mechanism in the decoder is modified to prevent positions from attending to subsequent positions. This masking ensures that predictions for a particular position only depend on the known outputs at positions before it.\n",
      "   - The decoder also includes residual connections and layer normalization in each layer.\n",
      "\n",
      "4. Attention Mechanism:\n",
      "   - The attention mechanism is a key component of the Transformer model. It allows the model to focus on different parts of the input sequence when generating an output. It can be described as mapping a query and a set of key-value pairs to an output.\n",
      "   - The attention mechanism computes the similarity between the query and each key and produces a weight for each key. These weights determine how much each value contributes to the output.\n",
      "   - The attention weights are computed using a dot product between the query and the keys, followed by a softmax function to obtain normalized weights.\n",
      "   - The attention mechanism is applied in parallel to multiple positions in the input sequence, resulting in multiple weighted sums that are concatenated and linearly projected to obtain the final output.\n",
      "\n",
      "5. Training and Inference:\n",
      "   - During training, the Transformer model is trained to minimize a loss function, such as cross-entropy loss, between the predicted output sequence and the target output sequence.\n",
      "   - Inference is performed by feeding the input sequence through the encoder to obtain the representation, and then using the decoder to generate the output sequence one element at a time. Beam search or other decoding strategies can be used to improve the quality of the generated output.\n",
      "\n",
      "Overall, the Transformer model replaces recurrent connections with self-attention mechanisms to capture dependencies between different positions in the input and output sequences. This allows for more parallelization and improved performance on sequence transduction tasks.\n"
     ]
    }
   ],
   "source": [
    "user_query = \"Explain the architecture and functioning of the Transformer model in detail?\"\n",
    "\n",
    "conversation_context = []\n",
    "\n",
    "response = qa_chain.run({\"question\": user_query, \"chat_history\": conversation_context})\n",
    "\n",
    "conversation_context.append({\"question\": user_query, \"answer\": response})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ac8acf-e516-4737-9021-8382fbcede4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
